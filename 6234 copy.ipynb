{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jdatetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(self):\n",
    "        self.url = None\n",
    "        self.df_title = None\n",
    "        self.df_location = None\n",
    "        self.df_date = None\n",
    "        self.buy_history = None\n",
    "        self.iter_history = None\n",
    "        self.interaction = None\n",
    "        self.merged_df = None\n",
    "        self.dfs = None\n",
    "        self.recommender = None\n",
    "        self.event_df = None\n",
    "\n",
    "    def data_scrap(self, url: str):\n",
    "        \"\"\"use site url to scrap necessary data\n",
    "\n",
    "        Args:\n",
    "            url (str): site address\n",
    "\n",
    "        \"\"\"\n",
    "        url = url\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            h3_tags_title = soup.find_all(\"h3\", class_=\"blog_post_title my-2\")\n",
    "            h3_tags_location = soup.find_all(\"div\", class_=\"blog_post_title my-2\")\n",
    "            h3_tags_date = soup.find_all(\"div\", class_=\"theater-date my-2\")\n",
    "\n",
    "            titles = []\n",
    "            location = []\n",
    "            date = []\n",
    "\n",
    "            for title in h3_tags_title:\n",
    "                if title.text.strip():\n",
    "                    titles.append(title.text.strip())\n",
    "\n",
    "            self.df_title = pd.DataFrame({\"Titles\": titles})\n",
    "            df_title = self.df_title\n",
    "\n",
    "            for loc in h3_tags_location:\n",
    "                if loc.text.strip():\n",
    "                    location.append(loc.text.strip())\n",
    "\n",
    "            self.df_location = pd.DataFrame({\"Titles\": location})\n",
    "            df_location = self.df_location\n",
    "\n",
    "            for dt in h3_tags_date:\n",
    "                if dt.text.strip():\n",
    "                    date.append(dt.text.strip())\n",
    "\n",
    "            self.df_date = pd.DataFrame({\"Titles\": date})\n",
    "            df_date = self.df_date\n",
    "\n",
    "            return df_title, df_location, df_date\n",
    "\n",
    "    def user_buy_interaction_from_api(self, buy_api: str, iter_api: str):\n",
    "        \"\"\"use api to scrap buy and interaction users\n",
    "\n",
    "        Args:\n",
    "            buy_api (str): api to scrape user buy history\n",
    "            iter_api (str): api to scrape user iter history\n",
    "        \"\"\"\n",
    "        pd.options.mode.copy_on_write = True\n",
    "        buy_link = buy_api\n",
    "        iter_link = iter_api\n",
    "\n",
    "        urllib.request.urlretrieve(iter_link, \"log.xlsx\")\n",
    "        self.iter_history = pd.read_excel(\"log.xlsx\")\n",
    "        iter_history = pd.read_excel(\"log.xlsx\")\n",
    "\n",
    "        urllib.request.urlretrieve(buy_link, \"visitor.xlsx\")\n",
    "        self.buy_history = pd.read_excel(\"visitor.xlsx\")\n",
    "        buy_history = pd.read_excel(\"visitor.xlsx\")\n",
    "\n",
    "        return iter_history, buy_history\n",
    "\n",
    "    def generate_date_ranges(self, start_date, end_date):\n",
    "        date_ranges = []\n",
    "        current_start_date = start_date\n",
    "        while current_start_date < end_date:\n",
    "            current_end_date = current_start_date + timedelta(days=60)\n",
    "            if current_end_date > end_date:\n",
    "                current_end_date = end_date\n",
    "            date_ranges.append((current_start_date, current_end_date))\n",
    "            current_start_date = current_end_date\n",
    "        return date_ranges\n",
    "\n",
    "    def fetch_data_from_api(self, url, start_date, end_date):\n",
    "        start_date = start_date.strftime(\"%Y/%m/%d\")\n",
    "        end_date = end_date.strftime(\"%Y/%m/%d\")\n",
    "        full_url = (\n",
    "            f\"{url}?token=aiapiqazxcvbnm1403&ofDate={start_date}&toDate={end_date}\"\n",
    "        )\n",
    "        response = requests.get(full_url)\n",
    "        try:\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            if response.text:  # Check if the response is not empty\n",
    "                data = response.json()\n",
    "                return pd.DataFrame(data)\n",
    "            else:\n",
    "                print(f\"Empty response for dates {start_date} to {end_date}\")\n",
    "                return pd.DataFrame()  # Return an empty DataFrame if no data\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTP error occurred: {e}\")\n",
    "            return pd.DataFrame()\n",
    "        except ValueError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def interaction_auto(self):\n",
    "        start_date_jalali = jdatetime.date(1402, 1, 1)\n",
    "        end_date_jalali = jdatetime.date.today()\n",
    "\n",
    "        date_ranges = self.generate_date_ranges(start_date_jalali, end_date_jalali)\n",
    "        all_data = []\n",
    "\n",
    "        for start, end in date_ranges:\n",
    "            df_log = self.fetch_data_from_api(\"https://6234.ir/api/log\", start, end)\n",
    "            all_data.append(df_log)\n",
    "\n",
    "        if all_data:\n",
    "            df_log_combined = pd.concat(all_data, ignore_index=True)\n",
    "            return df_log_combined\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def fetch_data_from_api_buy(self, start_date, end_date):\n",
    "        start_date = start_date.strftime(\"%Y/%m/%d\")\n",
    "        end_date = end_date.strftime(\"%Y/%m/%d\")\n",
    "        url = f\"https://6234.ir/api/ticket?token=aiapiqazxcvbnm1403&ofDate={start_date}&toDate={end_date}\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def buy_auto(self):\n",
    "        start_date_jalali = jdatetime.date(1402, 1, 1)\n",
    "        end_date_jalali = jdatetime.date.today()\n",
    "\n",
    "        date_ranges = self.generate_date_ranges(start_date_jalali, end_date_jalali)\n",
    "        all_data = []\n",
    "\n",
    "        for start, end in date_ranges:\n",
    "            df_buy = self.fetch_data_from_api(\"https://6234.ir/api/ticket\", start, end)\n",
    "            all_data.append(df_buy)\n",
    "\n",
    "        if all_data:\n",
    "            df_buy_combined = pd.concat(all_data, ignore_index=True)\n",
    "            return df_buy_combined\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "\n",
    "    def preprocessing_interaction(self, interaction_df: pd.DataFrame):\n",
    "        \"\"\"preprocessing interaction data for use in model\n",
    "\n",
    "        Args:\n",
    "            interaction_df (pd.DataFrame): interaction pd from user_buy_interaction_from_api func\n",
    "\n",
    "        Returns:\n",
    "            interaction_df (pd.DataFrame): interaction_df\n",
    "        \"\"\"\n",
    "        interaction_df[\"بازدید\"] = interaction_df[\"بازدید\"].fillna(\"ffill\")\n",
    "        interaction_df[\"نام و نام خانوادگی\"] = interaction_df[\n",
    "            \"نام و نام خانوادگی\"\n",
    "        ].fillna(\"none\")\n",
    "        interaction_df[\"شماره موبایل\"] = interaction_df[\"شماره موبایل\"].fillna(\"none\")\n",
    "        interaction_df = interaction_df[interaction_df[\"بازدید\"] != \"صفحه اصلی\"]\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        interaction_df.loc[:, \"userId\"] = le.fit_transform(\n",
    "            interaction_df[\"نام و نام خانوادگی\"]\n",
    "        )\n",
    "\n",
    "        return interaction_df\n",
    "\n",
    "    def event_api(self, api: str):\n",
    "        event_link = api\n",
    "\n",
    "        urllib.request.urlretrieve(event_link, \"event.xlsx\")\n",
    "        self.iter_history = pd.read_excel(\"event.xlsx\")\n",
    "        event_df = pd.read_excel(\"event.xlsx\")\n",
    "        event_df[\"Titles\"] = event_df[\"عنوان\"]\n",
    "\n",
    "        return event_df\n",
    "\n",
    "    def merged_all_df(\n",
    "        self,\n",
    "        df_title: pd.DataFrame,\n",
    "        df_location: pd.DataFrame,\n",
    "        df_date: pd.DataFrame,\n",
    "        df_interaction: pd.DataFrame,\n",
    "        df_buy_history: pd.DataFrame,\n",
    "        event_df: pd.DataFrame,\n",
    "    ):\n",
    "        \"\"\"merged all df to concat all titles under each other\n",
    "\n",
    "        Args:\n",
    "            df_title (pd.DataFrame): df_title scrape from data_scrap func output\n",
    "            df_location (pd.DataFrame): df_location scrape from data_scrap func output\n",
    "            df_date (pd.DataFrame): df_date scrape from data_scrap func output\n",
    "            df_interaction (pd.DataFrame): df_interaction scrape from user_buy_interaction_from_api func output\n",
    "            df_buy_history (pd.DataFrame): df_buy_history scrape from user_buy_interaction_from_api func output\n",
    "\n",
    "        Returns:\n",
    "            merged df: Pandas DataFrame\n",
    "        \"\"\"\n",
    "        merge_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Titles\": df_title[\"Titles\"],\n",
    "                \"Location\": df_location[\"Titles\"],\n",
    "                \"Date\": df_date[\"Titles\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        merge_df = pd.concat(\n",
    "            [\n",
    "                merge_df[\"Titles\"],\n",
    "                df_interaction[\"بازدید\"],\n",
    "                df_buy_history[\"رویداد\"],\n",
    "                event_df[\"Titles\"],\n",
    "            ]\n",
    "        ).reset_index()\n",
    "\n",
    "        merge_df.columns = [\"index\", \"Titles\"]\n",
    "\n",
    "        return merge_df\n",
    "\n",
    "    def list_to_string(self, row):\n",
    "        return \" \".join(row)\n",
    "\n",
    "    def remove_excel(self, excel_list: list):\n",
    "        for i in excel_list:\n",
    "            os.remove(i)\n",
    "\n",
    "    def preprocessing_merged_df(self, merged_df: pd.DataFrame):\n",
    "        \"\"\"preprocessing merged_df data for use in model\n",
    "\n",
    "        Args:\n",
    "            merged_df (pd.DataFrame): merged_df pd from merged_all_df func output\n",
    "\n",
    "        Returns:\n",
    "            merged_df: Pandas DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        df_ohe = merged_df[\"Titles\"].str.split(\" \").reset_index().astype(\"str\")\n",
    "        df_ohe[\"Titles\"] = df_ohe[\"Titles\"].apply(self.list_to_string)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        merged_df[\"ohe\"] = le.fit_transform(df_ohe[\"Titles\"])\n",
    "\n",
    "        self.merged_df = merged_df\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    def vectorized_text(self, df_title: pd.DataFrame):\n",
    "        \"\"\"vectorized_text for merged Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "        Args:\n",
    "            df_title (pd.DataFrame): use df_title from data_scrap func output\n",
    "\n",
    "        Returns:\n",
    "            X : array of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "\n",
    "        vectorized = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "        X = vectorized.fit_transform(self.merged_df[\"Titles\"])\n",
    "\n",
    "        feature_names = vectorized.get_feature_names_out()\n",
    "        one_hot_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "        dfs = pd.concat([df_title, one_hot_df], axis=1)\n",
    "        dfs.drop(columns=[\"Titles\"], inplace=True)\n",
    "\n",
    "        self.dfs = dfs\n",
    "        return dfs\n",
    "\n",
    "    def creat_X(self, interaction_df):\n",
    "        \"\"\"Compressed Sparse Row matrix.\n",
    "\n",
    "        Args:\n",
    "            iteraction_df (_type_): use preprocessing_interaction func output\n",
    "\n",
    "        Returns:\n",
    "            sparse matrix of type '<class 'numpy.float64'>\n",
    "        \"\"\"\n",
    "\n",
    "        M = interaction_df[\"userId\"].nunique()\n",
    "        N = interaction_df[\"بازدید\"].nunique()\n",
    "\n",
    "        user_mapper = dict(zip(np.unique(interaction_df[\"userId\"]), list(range(M))))\n",
    "        item_mapper = dict(zip(np.unique(interaction_df[\"بازدید\"]), list(range(N))))\n",
    "\n",
    "        user_inv_mapper = dict(zip(list(range(M)), np.unique(interaction_df[\"userId\"])))\n",
    "        item_inv_mapper = dict(zip(list(range(N)), np.unique(interaction_df[\"بازدید\"])))\n",
    "\n",
    "        user_index = [user_mapper[i] for i in interaction_df[\"userId\"]]\n",
    "        item_indx = [item_mapper[i] for i in interaction_df[\"بازدید\"]]\n",
    "\n",
    "        X = csr_matrix(\n",
    "            (interaction_df[\"زمان تعامل(تانیه)\"], (user_index, item_indx)), shape=(M, N)\n",
    "        )\n",
    "\n",
    "        return X, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper\n",
    "\n",
    "    def cosine_similioraty(\n",
    "        self,\n",
    "        dfs: pd.DataFrame,\n",
    "        event_df: pd.DataFrame,\n",
    "        interaction_df: pd.DataFrame,\n",
    "        idx: str,\n",
    "        n_recommendations: int = 1,\n",
    "    ):\n",
    "        \"\"\"Compute cosine similarity between samples in X and Y.\n",
    "\n",
    "        Cosine similarity, or the cosine kernel, computes similarity as the normalized dot product of X and Y:\n",
    "\n",
    "                Args:\n",
    "                    dfs (pd.DataFrame): use vectorized_text func outputs\n",
    "                    merged_df (pd.DataFrame): use preprocessing_merged_df func outputs\n",
    "                    interaction_df (pd.DataFrame): use preprocessing_interaction func output\n",
    "                    idx (str): idx of user interation and buy\n",
    "                    n_recommendations (int, optional): Number of outgoing recommenders. Defaults to 1.\n",
    "\n",
    "                Returns:\n",
    "                    list: user best recommenders\n",
    "        \"\"\"\n",
    "        cosine_sim = cosine_similarity(dfs, dfs)\n",
    "        iter_idx = dict(zip(event_df[\"Titles\"].unique(), list(event_df.index)))\n",
    "        idx = iter_idx[idx]\n",
    "        n_recommendations = n_recommendations\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1 : (n_recommendations + 1)]\n",
    "        similar_item = [i[0] for i in sim_scores]\n",
    "        recomended = event_df[\"Titles\"].iloc[similar_item]\n",
    "        recomended = recomended.to_list()\n",
    "\n",
    "        return recomended\n",
    "\n",
    "    def recomender_users(\n",
    "        self,\n",
    "        interaction_df: pd.DataFrame,\n",
    "        dfs: pd.DataFrame,\n",
    "        event_df: pd.DataFrame,\n",
    "        n_recommendations=1,\n",
    "    ):\n",
    "        \"\"\"use interaction_df ,dfs,merged_df to recommend best for each user\n",
    "\n",
    "        Args:\n",
    "            interaction_df (pd.DataFrame): use preprocessing_interaction func output\n",
    "            merged_df (pd.DataFrame): use preprocessing_merged_df func outputs\n",
    "            dfs (pd.DataFrame): use vectorized_text func outputs\n",
    "\n",
    "        Returns:\n",
    "            dict: user(phone number) recommender\n",
    "        \"\"\"\n",
    "        users_phone = interaction_df[\"شماره موبایل\"].unique()\n",
    "        user_iter = {}\n",
    "        for i in users_phone:\n",
    "            user_it = (\n",
    "                interaction_df[interaction_df[\"شماره موبایل\"] == i][\n",
    "                    [\"زمان تعامل(تانیه)\", \"بازدید\"]\n",
    "                ]\n",
    "                .max()\n",
    "                .reset_index()\n",
    "                .T\n",
    "            )\n",
    "            user_it.columns = [\"زمان تعامل(تانیه)\", \"بازدید\"]\n",
    "            user_it.drop(index=\"index\", inplace=True)\n",
    "            user_it[\"بازدید\"]\n",
    "            idx = user_it[\"بازدید\"].to_list()[0]\n",
    "            phone = str(i)\n",
    "            phone = phone[:-2]\n",
    "            iters = self.cosine_similioraty(\n",
    "                dfs,\n",
    "                event_df,\n",
    "                interaction_df,\n",
    "                idx=idx,\n",
    "                n_recommendations=n_recommendations,\n",
    "            )\n",
    "            user_dict = {phone: iters}\n",
    "            user_iter.update(user_dict)\n",
    "\n",
    "        temp = []\n",
    "        res = dict()\n",
    "\n",
    "        for key, val in user_iter.items():\n",
    "            if val not in res.values():\n",
    "                res[key] = val\n",
    "\n",
    "        return user_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomender = Recommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "interaction = recomender.interaction_auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\soheil\\anaconda3\\Lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\soheil\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\soheil\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\soheil\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m buy_history \u001b[38;5;241m=\u001b[39m recomender\u001b[38;5;241m.\u001b[39mbuy_auto()\n",
      "Cell \u001b[1;32mIn[2], line 132\u001b[0m, in \u001b[0;36mRecommender.buy_auto\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m all_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m date_ranges:\n\u001b[1;32m--> 132\u001b[0m     df_buy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_data_from_api_buy(start, end)\n\u001b[0;32m    133\u001b[0m     all_data\u001b[38;5;241m.\u001b[39mappend(df_buy)\n\u001b[0;32m    135\u001b[0m df_buy_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_data, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 121\u001b[0m, in \u001b[0;36mRecommender.fetch_data_from_api_buy\u001b[1;34m(self, start_date, end_date)\u001b[0m\n\u001b[0;32m    119\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m    120\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m--> 121\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "File \u001b[1;32mc:\\Users\\soheil\\anaconda3\\Lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "buy_history = recomender.buy_auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title, df_location, df_date = recomender.data_scrap(\"https://www.6234.ir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction, buy_history = recomender.user_buy_interaction_from_api(\n",
    "#     buy_api=\"https://6234.ir/api/ticket?token=aiapiqazxcvbnm1403&ofDate=1402/08/20&toDate=1403/12/29\",\n",
    "#     iter_api=\"https://6234.ir/api/log?token=aiapiqazxcvbnm1403&ofDate=1403/02/01&toDate=1403/03/29\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = recomender.event_api(\"https://6234.ir/api/event?token=aiapiqazxcvbnm1403\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction = recomender.preprocessing_interaction(interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = recomender.merged_all_df(\n",
    "    df_title, df_location, df_date, interaction, buy_history, event_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = recomender.preprocessing_merged_df(merged_df)\n",
    "dfs = recomender.vectorized_text(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper = recomender.creat_X(\n",
    "    interaction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': ['نمایش کمدی ژیلت',\n",
       "  'کنسرت هانیبال صلح و دوستی',\n",
       "  'کنسرت امید حاجیلی',\n",
       "  'قوانین و مقررات'],\n",
       " '9195920275': ['کنسرت نمایش کلنل',\n",
       "  'پارک امیرگان ( Test )',\n",
       "  'کنسرت تست ( با انتخاب صندلی )',\n",
       "  'نمایشگاه ترکیه']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomender.recomender_users(interaction, dfs, merged_df, n_recommendations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomender.remove_excel([\"event.xlsx\", \"log.xlsx\", \"visitor.xlsx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_ranges(start_date, end_date):\n",
    "    date_ranges = []\n",
    "    current_start_date = start_date\n",
    "    while current_start_date < end_date:\n",
    "        current_end_date = current_start_date + timedelta(days=60)\n",
    "        if current_end_date > end_date:\n",
    "            current_end_date = end_date\n",
    "        date_ranges.append((current_start_date, current_end_date))\n",
    "        current_start_date = current_end_date\n",
    "    return date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_from_api(start_date, end_date):\n",
    "    start_date = start_date.strftime(\"%Y/%m/%d\")\n",
    "    end_date = end_date.strftime(\"%Y/%m/%d\")\n",
    "    url = f\"https://6234.ir/api/log?token=aiapiqazxcvbnm1403&ofDate={start_date}&toDate={end_date}\"\n",
    "    urllib.request.urlretrieve(\n",
    "        url,\n",
    "        \"log.xlsx\",\n",
    "    )\n",
    "    iter_history = pd.read_excel(\"log.xlsx\")\n",
    "\n",
    "    return iter_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_jalali = jdatetime.datetime.strptime(\n",
    "    jdatetime.date(1402, 1, 1).strftime(\"%Y/%m/%d\"), \"%Y/%m/%d\"\n",
    ").date()\n",
    "end_date_jalali = jdatetime.datetime.strptime(\n",
    "    jdatetime.datetime.now().strftime(\"%Y/%m/%d\"), \"%Y/%m/%d\"\n",
    ").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(jdatetime.date(1402, 1, 1), jdatetime.date(1402, 2, 30)),\n",
       " (jdatetime.date(1402, 2, 30), jdatetime.date(1402, 4, 28)),\n",
       " (jdatetime.date(1402, 4, 28), jdatetime.date(1402, 6, 26)),\n",
       " (jdatetime.date(1402, 6, 26), jdatetime.date(1402, 8, 25)),\n",
       " (jdatetime.date(1402, 8, 25), jdatetime.date(1402, 10, 25)),\n",
       " (jdatetime.date(1402, 10, 25), jdatetime.date(1402, 12, 25)),\n",
       " (jdatetime.date(1402, 12, 25), jdatetime.date(1403, 2, 25)),\n",
       " (jdatetime.date(1403, 2, 25), jdatetime.date(1403, 3, 31))]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_ranges = generate_date_ranges(start_date_jalali, end_date_jalali)\n",
    "date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Excel file format cannot be determined, you must specify an engine manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m all_data \u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m date_ranges:\n\u001b[1;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m fetch_data_from_api(start, end)\n",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m, in \u001b[0;36mfetch_data_from_api\u001b[1;34m(start_date, end_date)\u001b[0m\n\u001b[0;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://6234.ir/api/log?token=aiapiqazxcvbnm1403&ofDate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&toDate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlretrieve(\n\u001b[0;32m      6\u001b[0m     url,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m iter_history \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iter_history\n",
      "File \u001b[1;32mc:\\Users\\soheil\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[0;32m    496\u001b[0m         io,\n\u001b[0;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\soheil\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1554\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1552\u001b[0m     )\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n\u001b[0;32m   1559\u001b[0m engine \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_option(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.excel.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.reader\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Excel file format cannot be determined, you must specify an engine manually."
     ]
    }
   ],
   "source": [
    "all_data = {}\n",
    "\n",
    "for start, end in date_ranges:\n",
    "    data = fetch_data_from_api(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1402/08/24 23:22:52\n",
       "1       1402/08/24 23:11:35\n",
       "2       1402/08/24 21:51:22\n",
       "3       1402/08/24 21:45:31\n",
       "4       1402/08/24 21:45:30\n",
       "               ...         \n",
       "2126    1402/07/20 19:46:31\n",
       "2127    1402/07/20 19:46:30\n",
       "2128    1402/07/20 19:46:28\n",
       "2129    1402/07/20 19:43:19\n",
       "2130    1402/07/20 19:32:59\n",
       "Name: تاریخ, Length: 2131, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"تاریخ\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402-01-01 1402-02-30\n",
      "1402-02-30 1402-04-28\n",
      "1402-04-28 1402-06-26\n",
      "1402-06-26 1402-08-25\n",
      "1402-08-25 1402-10-25\n",
      "1402-10-25 1402-12-25\n",
      "1402-12-25 1403-02-25\n",
      "1403-02-25 1403-03-31\n"
     ]
    }
   ],
   "source": [
    "for start, end in date_ranges:\n",
    "    print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jdatetime.date(1403, 2, 25)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jdatetime.date(1403, 3, 31)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['none', 9195920275.0], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction[\"شماره موبایل\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
