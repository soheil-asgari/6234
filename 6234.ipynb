{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jdatetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "from datetime import timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(self):\n",
    "        self.url = None\n",
    "        self.df_title = None\n",
    "        self.df_location = None\n",
    "        self.df_date = None\n",
    "        self.buy_history = None\n",
    "        self.iter_history = None\n",
    "        self.interaction = None\n",
    "        self.merged_df = None\n",
    "        self.dfs = None\n",
    "        self.recommender = None\n",
    "        self.event_df = None\n",
    "\n",
    "    def data_scrap(self, url: str):\n",
    "        \"\"\"use site url to scrap necessary data\n",
    "\n",
    "        Args:\n",
    "            url (str): site address\n",
    "\n",
    "        \"\"\"\n",
    "        url = url\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            h3_tags_title = soup.find_all(\"h3\", class_=\"blog_post_title my-2\")\n",
    "            h3_tags_location = soup.find_all(\"div\", class_=\"blog_post_title my-2\")\n",
    "            h3_tags_date = soup.find_all(\"div\", class_=\"theater-date my-2\")\n",
    "\n",
    "            titles = []\n",
    "            location = []\n",
    "            date = []\n",
    "\n",
    "            for title in h3_tags_title:\n",
    "                if title.text.strip():\n",
    "                    titles.append(title.text.strip())\n",
    "\n",
    "            self.df_title = pd.DataFrame({\"Titles\": titles})\n",
    "            df_title = self.df_title\n",
    "\n",
    "            for loc in h3_tags_location:\n",
    "                if loc.text.strip():\n",
    "                    location.append(loc.text.strip())\n",
    "\n",
    "            self.df_location = pd.DataFrame({\"Titles\": location})\n",
    "            df_location = self.df_location\n",
    "\n",
    "            for dt in h3_tags_date:\n",
    "                if dt.text.strip():\n",
    "                    date.append(dt.text.strip())\n",
    "\n",
    "            self.df_date = pd.DataFrame({\"Titles\": date})\n",
    "            df_date = self.df_date\n",
    "\n",
    "            return df_title, df_location, df_date\n",
    "\n",
    "    def user_buy_interaction_from_api(self, buy_api: str, iter_api: str):\n",
    "        \"\"\"use api to scrap buy and interaction users\n",
    "\n",
    "        Args:\n",
    "            buy_api (str): api to scrape user buy history\n",
    "            iter_api (str): api to scrape user iter history\n",
    "        \"\"\"\n",
    "        pd.options.mode.copy_on_write = True\n",
    "        buy_link = buy_api\n",
    "        iter_link = iter_api\n",
    "\n",
    "        urllib.request.urlretrieve(iter_link, \"log.xlsx\")\n",
    "        self.iter_history = pd.read_excel(\"log.xlsx\")\n",
    "        iter_history = pd.read_excel(\"log.xlsx\")\n",
    "\n",
    "        urllib.request.urlretrieve(buy_link, \"visitor.xlsx\")\n",
    "        self.buy_history = pd.read_excel(\"visitor.xlsx\")\n",
    "        buy_history = pd.read_excel(\"visitor.xlsx\")\n",
    "\n",
    "        return iter_history, buy_history\n",
    "\n",
    "    def generate_date_ranges(self, start_date, end_date):\n",
    "        date_ranges = []\n",
    "        current_start_date = start_date\n",
    "        while current_start_date < end_date:\n",
    "            current_end_date = current_start_date + timedelta(days=60)\n",
    "            if current_end_date > end_date:\n",
    "                current_end_date = end_date\n",
    "            date_ranges.append((current_start_date, current_end_date))\n",
    "            current_start_date = current_end_date\n",
    "        return date_ranges\n",
    "\n",
    "    def fetch_data_from_api_log(self, start_date, end_date):\n",
    "        start_date = start_date.strftime(\"%Y/%m/%d\")\n",
    "        end_date = end_date.strftime(\"%Y/%m/%d\")\n",
    "        url = f\"https://6234.ir/api/log?token=aiapiqazxcvbnm1403&ofDate={start_date}&toDate={end_date}\"\n",
    "        return url\n",
    "\n",
    "    def interaction_auto(self):\n",
    "        start_date_jalali = jdatetime.datetime.strptime(\n",
    "            jdatetime.date(1402, 1, 1).strftime(\"%Y/%m/%d\"), \"%Y/%m/%d\"\n",
    "        ).date()\n",
    "        end_date_jalali = jdatetime.datetime.strptime(\n",
    "            jdatetime.datetime.now().strftime(\"%Y/%m/%d\"), \"%Y/%m/%d\"\n",
    "        ).date()\n",
    "        date_ranges = self.generate_date_ranges(start_date_jalali, end_date_jalali)\n",
    "        for start, end in date_ranges:\n",
    "            urllib.request.urlretrieve(\n",
    "                self.fetch_data_from_api_log(start, end), f\"log{start.month}.xlsx\"\n",
    "            )\n",
    "\n",
    "        df_api = {}\n",
    "        for start, end in date_ranges:\n",
    "            month = start.month\n",
    "            df = pd.read_excel(f\"log{month}.xlsx\")\n",
    "\n",
    "            if month in df_api:\n",
    "\n",
    "                df_api[month] = pd.concat([df_api[month], df], ignore_index=True)\n",
    "            else:\n",
    "                df_api[month] = df\n",
    "\n",
    "        combined_df = pd.concat(df_api.values(), ignore_index=True)\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def fetch_data_from_api_buy(self, start_date, end_date):\n",
    "        start_date = start_date.strftime(\"%Y/%m/%d\")\n",
    "        end_date = end_date.strftime(\"%Y/%m/%d\")\n",
    "        url = f\"https://6234.ir/api/ticket?token=aiapiqazxcvbnm1403&ofDate={start_date}&toDate={end_date}\"\n",
    "        return url\n",
    "\n",
    "    def buy_auto(self):\n",
    "        start_date_jalali = jdatetime.datetime.strptime(\n",
    "            jdatetime.date(1402, 1, 1).strftime(\"%Y/%m/%d\"), \"%Y/%m/%d\"\n",
    "        ).date()\n",
    "        end_date_jalali = jdatetime.datetime.strptime(\n",
    "            jdatetime.datetime.now().strftime(\"%Y/%m/%d\"), \"%Y/%m/%d\"\n",
    "        ).date()\n",
    "        date_ranges = self.generate_date_ranges(start_date_jalali, end_date_jalali)\n",
    "        for start, end in date_ranges:\n",
    "            urllib.request.urlretrieve(\n",
    "                self.fetch_data_from_api_buy(start, end), f\"log{start.month}.xlsx\"\n",
    "            )\n",
    "\n",
    "        df_api = {}\n",
    "        for start, end in date_ranges:\n",
    "            month = start.month\n",
    "            df = pd.read_excel(f\"log{month}.xlsx\")\n",
    "\n",
    "            if month in df_api:\n",
    "\n",
    "                df_api[month] = pd.concat([df_api[month], df], ignore_index=True)\n",
    "            else:\n",
    "                df_api[month] = df\n",
    "\n",
    "        combined_df = pd.concat(df_api.values(), ignore_index=True)\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def preprocessing_interaction(self, interaction_df: pd.DataFrame):\n",
    "        \"\"\"preprocessing interaction data for use in model\n",
    "\n",
    "        Args:\n",
    "            interaction_df (pd.DataFrame): interaction pd from user_buy_interaction_from_api func\n",
    "\n",
    "        Returns:\n",
    "            interaction_df (pd.DataFrame): interaction_df\n",
    "        \"\"\"\n",
    "        interaction_df[\"بازدید\"] = interaction_df[\"بازدید\"].fillna(\"ffill\")\n",
    "        interaction_df[\"نام و نام خانوادگی\"] = interaction_df[\n",
    "            \"نام و نام خانوادگی\"\n",
    "        ].fillna(\"none\")\n",
    "        interaction_df[\"شماره موبایل\"] = interaction_df[\"شماره موبایل\"].fillna(\"none\")\n",
    "        interaction_df = interaction_df[interaction_df[\"بازدید\"] != \"صفحه اصلی\"]\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        interaction_df.loc[:, \"userId\"] = le.fit_transform(\n",
    "            interaction_df[\"نام و نام خانوادگی\"]\n",
    "        )\n",
    "\n",
    "        return interaction_df\n",
    "\n",
    "    def event_api(self, api: str):\n",
    "        event_link = api\n",
    "\n",
    "        urllib.request.urlretrieve(event_link, \"event.xlsx\")\n",
    "        self.iter_history = pd.read_excel(\"event.xlsx\")\n",
    "        event_df = pd.read_excel(\"event.xlsx\")\n",
    "        event_df[\"Titles\"] = event_df[\"عنوان\"]\n",
    "\n",
    "        return event_df\n",
    "\n",
    "    def merged_all_df(\n",
    "        self,\n",
    "        df_title: pd.DataFrame,\n",
    "        df_location: pd.DataFrame,\n",
    "        df_date: pd.DataFrame,\n",
    "        df_interaction: pd.DataFrame,\n",
    "        # df_buy_history: pd.DataFrame,\n",
    "        event_df: pd.DataFrame,\n",
    "    ):\n",
    "        \"\"\"merged all df to concat all titles under each other\n",
    "\n",
    "        Args:\n",
    "            df_title (pd.DataFrame): df_title scrape from data_scrap func output\n",
    "            df_location (pd.DataFrame): df_location scrape from data_scrap func output\n",
    "            df_date (pd.DataFrame): df_date scrape from data_scrap func output\n",
    "            df_interaction (pd.DataFrame): df_interaction scrape from user_buy_interaction_from_api func output\n",
    "            df_buy_history (pd.DataFrame): df_buy_history scrape from user_buy_interaction_from_api func output\n",
    "\n",
    "        Returns:\n",
    "            merged df: Pandas DataFrame\n",
    "        \"\"\"\n",
    "        merge_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Titles\": df_title[\"Titles\"],\n",
    "                \"Location\": df_location[\"Titles\"],\n",
    "                \"Date\": df_date[\"Titles\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        merge_df = pd.concat(\n",
    "            [\n",
    "                merge_df[\"Titles\"],\n",
    "                df_interaction[\"بازدید\"],\n",
    "                # df_buy_history[\"رویداد\"],\n",
    "                event_df[\"Titles\"],\n",
    "            ]\n",
    "        ).reset_index()\n",
    "\n",
    "        merge_df.columns = [\"index\", \"Titles\"]\n",
    "\n",
    "        return merge_df\n",
    "\n",
    "    def list_to_string(self, row):\n",
    "        return \" \".join(row)\n",
    "\n",
    "    def remove_excel(self, excel_list: list):\n",
    "        for i in excel_list:\n",
    "            os.remove(i)\n",
    "\n",
    "    def preprocessing_merged_df(self, merged_df: pd.DataFrame):\n",
    "        \"\"\"preprocessing merged_df data for use in model\n",
    "\n",
    "        Args:\n",
    "            merged_df (pd.DataFrame): merged_df pd from merged_all_df func output\n",
    "\n",
    "        Returns:\n",
    "            merged_df: Pandas DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        df_ohe = merged_df[\"Titles\"].str.split(\" \").reset_index().astype(\"str\")\n",
    "        df_ohe[\"Titles\"] = df_ohe[\"Titles\"].apply(self.list_to_string)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        merged_df[\"ohe\"] = le.fit_transform(df_ohe[\"Titles\"])\n",
    "\n",
    "        self.merged_df = merged_df\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    def vectorized_text(self, df_title: pd.DataFrame):\n",
    "        \"\"\"vectorized_text for merged Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "        Args:\n",
    "            df_title (pd.DataFrame): use df_title from data_scrap func output\n",
    "\n",
    "        Returns:\n",
    "            X : array of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "\n",
    "        vectorized = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "        X = vectorized.fit_transform(self.merged_df[\"Titles\"])\n",
    "\n",
    "        feature_names = vectorized.get_feature_names_out()\n",
    "        one_hot_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "        dfs = pd.concat([df_title, one_hot_df], axis=1)\n",
    "        dfs.drop(columns=[\"Titles\"], inplace=True)\n",
    "\n",
    "        self.dfs = dfs\n",
    "        return dfs\n",
    "\n",
    "    def creat_X(self, interaction_df):\n",
    "        \"\"\"Compressed Sparse Row matrix.\n",
    "\n",
    "        Args:\n",
    "            iteraction_df (_type_): use preprocessing_interaction func output\n",
    "\n",
    "        Returns:\n",
    "            sparse matrix of type '<class 'numpy.float64'>\n",
    "        \"\"\"\n",
    "\n",
    "        M = interaction_df[\"userId\"].nunique()\n",
    "        N = interaction_df[\"بازدید\"].nunique()\n",
    "\n",
    "        user_mapper = dict(zip(np.unique(interaction_df[\"userId\"]), list(range(M))))\n",
    "        item_mapper = dict(zip(np.unique(interaction_df[\"بازدید\"]), list(range(N))))\n",
    "\n",
    "        user_inv_mapper = dict(zip(list(range(M)), np.unique(interaction_df[\"userId\"])))\n",
    "        item_inv_mapper = dict(zip(list(range(N)), np.unique(interaction_df[\"بازدید\"])))\n",
    "\n",
    "        user_index = [user_mapper[i] for i in interaction_df[\"userId\"]]\n",
    "        item_indx = [item_mapper[i] for i in interaction_df[\"بازدید\"]]\n",
    "\n",
    "        X = csr_matrix(\n",
    "            (interaction_df[\"زمان تعامل(تانیه)\"], (user_index, item_indx)), shape=(M, N)\n",
    "        )\n",
    "\n",
    "        return X, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper\n",
    "\n",
    "    def cosine_similioraty(\n",
    "        self,\n",
    "        dfs: pd.DataFrame,\n",
    "        event_df: pd.DataFrame,\n",
    "        interaction_df: pd.DataFrame,\n",
    "        idx: str,\n",
    "        n_recommendations: int = 1,\n",
    "    ):\n",
    "        \"\"\"Compute cosine similarity between samples in X and Y.\n",
    "\n",
    "        Cosine similarity, or the cosine kernel, computes similarity as the normalized dot product of X and Y:\n",
    "\n",
    "                Args:\n",
    "                    dfs (pd.DataFrame): use vectorized_text func outputs\n",
    "                    merged_df (pd.DataFrame): use preprocessing_merged_df func outputs\n",
    "                    interaction_df (pd.DataFrame): use preprocessing_interaction func output\n",
    "                    idx (str): idx of user interation and buy\n",
    "                    n_recommendations (int, optional): Number of outgoing recommenders. Defaults to 1.\n",
    "\n",
    "                Returns:\n",
    "                    list: user best recommenders\n",
    "        \"\"\"\n",
    "        cosine_sim = cosine_similarity(dfs, dfs)\n",
    "        iter_idx = dict(zip(event_df[\"Titles\"].unique(), list(event_df.index)))\n",
    "        idx = iter_idx[idx]\n",
    "        n_recommendations = n_recommendations\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1 : (n_recommendations + 1)]\n",
    "        similar_item = [i[0] for i in sim_scores]\n",
    "        recomended = event_df[\"Titles\"].iloc[similar_item]\n",
    "        recomended = recomended.to_list()\n",
    "\n",
    "        return recomended\n",
    "\n",
    "    def recomender_users(\n",
    "        self,\n",
    "        interaction_df: pd.DataFrame,\n",
    "        dfs: pd.DataFrame,\n",
    "        event_df: pd.DataFrame,\n",
    "        n_recommendations=1,\n",
    "    ):\n",
    "        \"\"\"use interaction_df, dfs, event_df to recommend best for each user\n",
    "\n",
    "        Args:\n",
    "            interaction_df (pd.DataFrame): output of preprocessing_interaction function\n",
    "            dfs (pd.DataFrame): output of vectorized_text function\n",
    "            event_df (pd.DataFrame): output of event_api function\n",
    "            n_recommendations (int, optional): number of recommendations per user. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            dict: user(phone number) recommender\n",
    "        \"\"\"\n",
    "        users_phone = interaction_df[\"شماره موبایل\"].unique()\n",
    "        user_recommendations = {}\n",
    "\n",
    "        for phone_number in users_phone:\n",
    "            user_data = (\n",
    "                interaction_df[interaction_df[\"شماره موبایل\"] == phone_number][\n",
    "                    [\"زمان تعامل(تانیه)\", \"بازدید\"]\n",
    "                ]\n",
    "                .max()\n",
    "                .reset_index()\n",
    "                .T\n",
    "            )\n",
    "            user_data.columns = [\"زمان تعامل(تانیه)\", \"بازدید\"]\n",
    "            user_data.drop(index=\"index\", inplace=True)\n",
    "            idx = user_data[\"بازدید\"].tolist()[0]\n",
    "\n",
    "            recommendations = self.cosine_similioraty(\n",
    "                dfs,\n",
    "                event_df,\n",
    "                interaction_df,\n",
    "                idx=idx,\n",
    "                n_recommendations=n_recommendations + 1,\n",
    "            )\n",
    "\n",
    "            # Remove duplicates from recommendations\n",
    "            unique_recommendations = list(dict.fromkeys(recommendations))\n",
    "\n",
    "            # Keep only the first n_recommendations unique recommendations\n",
    "            final_recommendations = []\n",
    "            seen = set()\n",
    "            for rec in recommendations:\n",
    "                if rec not in seen:\n",
    "                    final_recommendations.append(rec)\n",
    "                    seen.add(rec)\n",
    "                if len(final_recommendations) == n_recommendations:\n",
    "                    break\n",
    "\n",
    "            user_recommendations[phone_number] = final_recommendations\n",
    "\n",
    "        return user_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomender = Recommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soheil\\AppData\\Local\\Temp\\ipykernel_7020\\1487782170.py:122: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(df_api.values(), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "interaction = recomender.interaction_auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soheil\\AppData\\Local\\Temp\\ipykernel_7020\\1487782170.py:156: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(df_api.values(), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "buy_history = recomender.buy_auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title, df_location, df_date = recomender.data_scrap(\"https://www.6234.ir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = recomender.event_api(\"https://6234.ir/api/event?token=aiapiqazxcvbnm1403\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soheil\\AppData\\Local\\Temp\\ipykernel_7020\\1487782170.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  interaction_df.loc[:, \"userId\"] = le.fit_transform(\n"
     ]
    }
   ],
   "source": [
    "interaction = recomender.preprocessing_interaction(interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df_ = event_df[[\"Titles\"]]\n",
    "event_df_ = event_df_.dropna()\n",
    "event = event_df_[\"Titles\"]\n",
    "filtered_interaction = interaction[interaction[\"بازدید\"].isin(event.tolist())]\n",
    "merged_df = recomender.merged_all_df(\n",
    "    df_title, df_location, df_date, filtered_interaction, event_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df_ = event_df[[\"Titles\"]]\n",
    "event_df_ = event_df_.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = recomender.preprocessing_merged_df(merged_df)\n",
    "dfs = recomender.vectorized_text(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper = recomender.creat_X(\n",
    "    interaction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'none': ['کنسرت نمایش کلنل', 'پارک امیرگان ( Test )'],\n",
       " 9195920275.0: ['کنسرت نمایش کلنل', 'پارک امیرگان ( Test )'],\n",
       " 9101758962.0: ['مسابقه آنلاین', 'تور چابهار مکران'],\n",
       " 9117803218.0: ['پارکینگ VIP', 'نمایشگاه گردشگری در فضای مجازی'],\n",
       " 9101022210.0: ['نمایشگاه ترکیه', 'مسابقه آنلاین'],\n",
       " 9102331142.0: ['نمایشگاه جشنواره هوش مصنوعی در تبلیغات و اطلاع رسانی',\n",
       "  'نمایشگاه گردشگری در فضای مجازی']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomender.recomender_users(filtered_interaction, dfs, merged_df, n_recommendations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
